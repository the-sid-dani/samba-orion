# ğŸ¯ **Langfuse + Vercel AI SDK Integration**

## âœ… **Current Implementation - NodeTracerProvider + Langfuse SDK**

This document outlines the **production implementation** for Langfuse integration in Better Chatbot using the Langfuse SDK v4 with OpenTelemetry.

## ğŸ—ï¸ **Production Architecture**

### **1. Package Setup**
```bash
# Production packages for Vercel AI SDK v5 + Langfuse SDK v4
npm install @langfuse/otel @langfuse/tracing @opentelemetry/sdk-trace-node @opentelemetry/api
```

**IMPORTANT:** The `langfuse-vercel` + `@vercel/otel` approach does **NOT** support Vercel AI SDK v2+ (which includes v5). Use the NodeTracerProvider approach documented below.

### **2. Instrumentation Setup** (`instrumentation.ts`)
```typescript
import { LangfuseSpanProcessor, ShouldExportSpan } from "@langfuse/otel";
import { NodeTracerProvider } from "@opentelemetry/sdk-trace-node";

// Validate required environment variables at startup
function validateLangfuseConfig() {
  const missing = [];
  if (!process.env.LANGFUSE_PUBLIC_KEY) missing.push("LANGFUSE_PUBLIC_KEY");
  if (!process.env.LANGFUSE_SECRET_KEY) missing.push("LANGFUSE_SECRET_KEY");

  if (missing.length > 0) {
    console.error("âš ï¸ Missing Langfuse variables:", missing.join(", "));
    console.error("Traces will NOT be sent to Langfuse");
  } else {
    const baseUrl = process.env.LANGFUSE_BASE_URL || "https://cloud.langfuse.com";
    const environment = process.env.VERCEL_ENV || process.env.NODE_ENV || "development";
    console.log(`âœ… Langfuse configured: ${baseUrl} [${environment}]`);
  }
}

validateLangfuseConfig();

// Filter out Next.js infrastructure spans
const shouldExportSpan: ShouldExportSpan = (span) => {
  return span.otelSpan.instrumentationScope.name !== "next.js";
};

export const langfuseSpanProcessor = new LangfuseSpanProcessor({
  shouldExportSpan,
});

const tracerProvider = new NodeTracerProvider({
  spanProcessors: [langfuseSpanProcessor],
});

tracerProvider.register();
```

### **3. Chat API Integration** (`/api/chat/route.ts`)

Use Langfuse SDK's `observe()` wrapper with `experimental_telemetry` for complete tracing:

```typescript
import { observe, updateActiveTrace, updateActiveObservation } from "@langfuse/tracing";
import { trace } from "@opentelemetry/api";
import { after } from "next/server";
import { langfuseSpanProcessor } from "@/instrumentation";

// Wrap the handler with observe()
const handler = async (request: Request) => {
  // ... your chat logic ...

  // Set trace metadata early
  const environment = process.env.VERCEL_ENV || process.env.NODE_ENV || "development";
  const mcpClients = await mcpClientsManager.getClients();
  const mcpServerList = mcpClients.map((client) => client.serverName);

  updateActiveTrace({
    name: agent?.name ? `agent-${agent.name}-chat` : "samba-orion-chat",
    sessionId: id,
    userId: session.user.id,
    input: inputText,
    metadata: {
      agentId: agent?.id,
      agentName: agent?.name,
      provider: chatModel?.provider,
      model: chatModel?.model,
      toolChoice,
      environment,
      mcpServerCount: mcpClients.length,
      mcpServerList,
      tags: [
        "chat",
        `provider:${chatModel?.provider || "unknown"}`,
        `model:${chatModel?.model || "unknown"}`,
        ...(agent?.name ? [`agent:${agent.name}`] : []),
        `environment:${environment}`,
      ],
    },
  });

  // Enable telemetry in streamText
  const result = streamText({
    model,
    system: systemPrompt,
    messages: convertToModelMessages(messages),
    experimental_telemetry: {
      isEnabled: true,
    },
    tools: vercelAITooles,
    onFinish: async (result) => {
      // Add tool execution metadata
      const toolExecutionCount = result.steps?.reduce((count, step) => {
        return count + (step.toolCalls?.length || 0) + (step.toolResults?.length || 0);
      }, 0) || 0;

      updateActiveTrace({
        output: result.content,
        metadata: {
          toolExecutionCount,
          mcpToolCount: Object.keys(MCP_TOOLS ?? {}).length,
          workflowToolCount: Object.keys(WORKFLOW_TOOLS ?? {}).length,
          appToolCount: Object.keys(APP_DEFAULT_TOOLS ?? {}).length,
        },
      });

      // End span manually after stream has finished
      trace.getActiveSpan()?.end();
    },
  });

  // CRITICAL: Force flush in serverless environments
  after(async () => {
    await langfuseSpanProcessor.forceFlush();
  });

  return createUIMessageStreamResponse({ stream });
};

// Export the wrapped handler
export const POST = observe(handler, {
  name: "chat-api-handler",
  endOnExit: false, // end observation _after_ stream has finished
});
```

## ğŸ¯ **What This Approach Captures**

### **Automatic Tracing via `experimental_telemetry`**
- âœ… **LLM Calls**: All `streamText`, `generateText` calls automatically traced via OpenTelemetry
- âœ… **Tool Execution**: Individual tool calls within the AI SDK automatically captured
- âœ… **Streaming Responses**: Real-time streaming traces with proper flushing
- âœ… **Token Usage**: Input/output tokens automatically recorded
- âœ… **Costs**: Token costs calculated automatically by Langfuse
- âœ… **Errors**: Failures and exceptions automatically captured
- âœ… **Multi-Provider**: Works with ALL Vercel AI SDK providers (OpenAI, Anthropic, Google, xAI, Ollama, OpenRouter)

### **CRITICAL: Serverless Flush Requirement**
**REQUIRED for Vercel/serverless deployments:** You MUST call `langfuseSpanProcessor.forceFlush()` to ensure traces are sent before the function terminates. Use Next.js `after()` for background flushing without blocking the response.

### **Rich Metadata Captured**
- ğŸ‘¤ **User Context**: userId, sessionId for user journey tracking
- ğŸ¤– **Agent Context**: agentId, agentName, agent-specific trace names
- ğŸ”§ **Tool Context**: toolExecutionCount, mcpServerCount, mcpServerList, tool breakdown
- ğŸ“Š **Performance**: Token usage, latency, costs per conversation
- ğŸ·ï¸ **Tags**: Custom tags for filtering and analytics (provider, model, agent, environment)
- ğŸŒ **Environment**: Automatic production/preview/development detection via VERCEL_ENV

## ğŸ‰ **Key Benefits of This Approach**

### **1. Production-Ready for Vercel AI SDK v5**
- âœ… **Compatible with AI SDK v5** - Uses NodeTracerProvider (langfuse-vercel is incompatible)
- âœ… **Explicit flush control** - Required for serverless, handled via `forceFlush()` + `after()`
- âœ… **Multi-provider support** - Works with ALL your AI providers
- âœ… **Tool execution tracing** - MCP, Workflow, App Default tools captured
- âœ… **Streaming optimized** - Real-time response tracing with proper lifecycle management

### **2. Comprehensive Observability**
- âœ… **Environment validation** - Startup checks ensure credentials are configured
- âœ… **Agent-specific traces** - Trace names reflect which agent handled the conversation
- âœ… **Tool execution metrics** - Track how many tools executed per conversation
- âœ… **Error handling** - Failures automatically captured with context
- âœ… **Health monitoring** - `/api/health/langfuse/traces` endpoint for production monitoring

## ğŸ“Š **What You'll See in Langfuse**

### **IMPORTANT: Understanding Traces vs Sessions**

**Each message = One trace** (this is expected behavior):
- Every POST to `/api/chat` creates a new trace via the `observe()` wrapper
- All messages with the same `sessionId` are linked together
- **View conversations in the "Sessions" tab in Langfuse**, not the "Traces" tab

**Why this architecture?**
- Serverless functions are stateless - each API call is independent
- The `observe()` wrapper creates one trace per function invocation
- `sessionId` links all traces in a conversation for analysis

**How to view grouped conversations:**
1. Go to **Sessions tab** in Langfuse dashboard
2. Filter by `sessionId` (your chat thread ID)
3. See all messages in that conversation grouped together

### **Trace Structure (Per Message)**
```
agent-CodeAssistant-chat (or samba-orion-chat)
â”œâ”€â”€ chat-api-handler (observe wrapper)
â”‚   â””â”€â”€ ai.streamText.doStream (automatic via experimental_telemetry)
â”‚       â”œâ”€â”€ ai.streamText.doStream.startStep (automatic)
â”‚       â”œâ”€â”€ ai.toolCall.mcp__tool_name (automatic for each MCP tool)
â”‚       â”œâ”€â”€ ai.toolCall.workflow__tool_name (automatic for each workflow tool)
â”‚       â””â”€â”€ ai.streamText.doStream.finishStep (automatic)
â””â”€â”€ metadata: userId, sessionId, threadId, messageId, agent, environment, tools, execution counts
```

### **Captured Metrics**
- **Conversation Analytics**: User sessions, agent usage patterns, environment breakdown
- **Model Performance**: Token usage, latency, costs per provider
- **Tool Usage**: toolExecutionCount, mcpToolCount, workflowToolCount, appToolCount
- **Agent Effectiveness**: Agent-specific trace names for filtering and comparison
- **Cost Intelligence**: Real-time cost tracking across all providers
- **Environment Tracking**: Production vs preview vs development trace segregation

## ğŸ”§ **Configuration**

### **Environment Variables**
```bash
# Required
LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key-here
LANGFUSE_SECRET_KEY=sk-lf-your-secret-key-here

# Optional (defaults to cloud.langfuse.com)
LANGFUSE_BASE_URL=https://cloud.langfuse.com

# Automatic (set by Vercel)
VERCEL_ENV=production  # or preview, development
NODE_ENV=production    # Fallback if VERCEL_ENV not set
```

### **Next.js Configuration**
Next.js 15 supports `instrumentation.ts` by default. No configuration needed.

For Next.js < 15, add to `next.config.js`:
```javascript
module.exports = {
  experimental: {
    instrumentationHook: true,
  },
};
```

## ğŸš€ **How It Works**

1. **Next.js loads** `instrumentation.ts` automatically on startup
2. **Environment validation** checks for required Langfuse credentials
3. **NodeTracerProvider** sets up OpenTelemetry with LangfuseSpanProcessor
4. **`observe()` wrapper** creates trace context for each API request
5. **`experimental_telemetry`** enables automatic span creation for AI operations
6. **`updateActiveTrace()`** adds custom metadata to traces
7. **`forceFlush()`** ensures traces are sent before serverless function terminates
8. **LangfuseSpanProcessor** sends all spans to Langfuse dashboard

## âœ… **Verification Steps**

1. **Check startup logs** - Should see `âœ… Langfuse configured: [baseUrl] [environment]`
2. **Start development server**: `pnpm dev`
3. **Send a chat message** through your app
4. **Check Langfuse dashboard** - traces should appear within 30 seconds
5. **Verify metadata** - Check for userId, agent names, tool counts, environment tags, threadId, messageId
6. **Check Sessions view** - Go to Sessions tab and filter by sessionId to see grouped conversations
7. **Test health endpoint**: `curl http://localhost:3000/api/health/langfuse/traces`

## ğŸ› **Troubleshooting**

### **Issue: Empty/Ghost Traces**

**Symptom:** Traces with `null` input/output and empty metadata

**Cause:** The `observe()` wrapper creates a trace immediately, but if the request fails early (auth errors, validation errors), metadata isn't set.

**Fix Applied:**
- Trace metadata is now set immediately at handler start (before any early returns)
- Early returns (401, 403) now include error metadata
- Every trace will have at minimum: userId, environment, and error info if applicable

**Verification:**
```bash
# Check traces - should have metadata even for errors
# No more empty traces with null input/output
```

### **Issue: Each Message is a Separate Trace**

**Symptom:** Conversation with 3 messages shows as 3 separate traces

**This is EXPECTED behavior:**
- Each POST request = one trace (by design)
- Messages are linked via `sessionId` metadata
- **Solution:** Use the **Sessions tab** in Langfuse to view grouped conversations

**How to view conversations:**
1. Open Langfuse dashboard
2. Click "Sessions" tab (not "Traces")
3. Find your session by ID or filter by user
4. All traces for that conversation will be grouped together

### **Issue: Traces Not Appearing in Langfuse**

**Possible causes:**
1. Missing environment variables (check startup logs)
2. Flush not completing in serverless (already handled via `forceFlush()` + `after()`)
3. Network connectivity to Langfuse cloud/self-hosted instance

**Debug steps:**
```bash
# 1. Check environment variables
echo $LANGFUSE_PUBLIC_KEY
echo $LANGFUSE_SECRET_KEY
echo $LANGFUSE_BASE_URL

# 2. Check health endpoint
curl http://localhost:3000/api/health/langfuse/traces

# 3. Check server logs for Langfuse errors
pnpm dev | grep -i langfuse
```

## ğŸ” **Health Monitoring**

### **Basic Health Check**
```bash
GET /api/health/langfuse
```
Returns: Connectivity status and credential configuration

### **Trace Health Check**
```bash
GET /api/health/langfuse/traces
```
Returns:
- Last trace sent timestamp
- Trace count in last hour
- Flush status
- Connection health
- Configuration validation

## ğŸ¯ **Implementation Summary**

**This implementation is production-ready** and provides:
- âœ… Compatible with Vercel AI SDK v5.0.26
- âœ… Works with Langfuse SDK v4.1.0
- âœ… Supports multi-agent platform with agent-specific tracing
- âœ… Comprehensive tool execution tracking (MCP, Workflow, App tools)
- âœ… Proper serverless flush handling via `forceFlush()` + `after()`
- âœ… Environment validation with clear error messages
- âœ… Production/preview/development environment segregation
- âœ… Health monitoring endpoints for production observability

**Key Differences from langfuse-vercel approach:**
- Uses `@langfuse/otel` + `@langfuse/tracing` instead of `langfuse-vercel`
- Requires explicit `forceFlush()` calls (handled automatically with `after()`)
- Compatible with Vercel AI SDK v2+ (langfuse-vercel only supports v1)
- More control over trace lifecycle and metadata